{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df60cfbf-5b58-46c3-aae5-842ae38b1114",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Basics of Spark Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838f93e6-5732-4b6d-b111-aa5526ea79e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"SparkDemo\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5899f853-42c6-40ea-9116-eff10c993c32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+-------+-------------------+----------+---+\n|order_id|prod_id|unit_price|qty|prod_id|          prod_name|list_price|qty|\n+--------+-------+----------+---+-------+-------------------+----------+---+\n|      03|     01|       195|  1|     01|       Scroll Mouse|       250| 20|\n|      01|     02|       350|  1|     02|      Optical Mouse|       350| 20|\n|      05|     02|       350|  1|     02|      Optical Mouse|       350| 20|\n|      02|     03|       450|  1|     03|     Wireless Mouse|       450| 50|\n|      01|     04|       580|  1|     04|  Wireless Keyboard|       580| 50|\n|      02|     06|       220|  1|     06|16 GB Flash Storage|       240|100|\n|      01|     07|       320|  2|     07|32 GB Flash Storage|       320| 50|\n|      04|     08|       410|  2|     08|64 GB Flash Storage|       430| 25|\n+--------+-------+----------+---+-------+-------------------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "orders_list = [(\"01\", \"02\", 350, 1),\n",
    "                (\"01\", \"04\", 580, 1),\n",
    "                (\"01\", \"07\", 320, 2),\n",
    "                (\"02\", \"03\", 450, 1),\n",
    "                (\"02\", \"06\", 220, 1),\n",
    "                (\"03\", \"01\", 195, 1),\n",
    "                (\"04\", \"09\", 270, 3),\n",
    "                (\"04\", \"08\", 410, 2),\n",
    "                (\"05\", \"02\", 350, 1)]\n",
    "\n",
    "order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
    "\n",
    "product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
    "                (\"02\", \"Optical Mouse\", 350, 20), \n",
    "                (\"03\", \"Wireless Mouse\", 450, 50),\n",
    "                (\"04\", \"Wireless Keyboard\", 580, 50),\n",
    "                (\"05\", \"Standard Keyboard\", 360, 10),\n",
    "                (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
    "                (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
    "                (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
    "\n",
    "product_df = spark.createDataFrame (product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
    "\n",
    "# product_df.show()\n",
    "# order_df.show()\n",
    "\n",
    "# INNER JOIN\n",
    "\n",
    "join_expr = order_df.prod_id == product_df.prod_id\n",
    "\n",
    "order_df.join(product_df, join_expr, \"inner\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eeafeed-73d4-404d-b11f-db54f2a29fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+----------+---+\n|order_id|prod_id|          prod_name|unit_price|qty|\n+--------+-------+-------------------+----------+---+\n|      03|     01|       Scroll Mouse|       195|  1|\n|      01|     02|      Optical Mouse|       350|  1|\n|      05|     02|      Optical Mouse|       350|  1|\n|      02|     03|     Wireless Mouse|       450|  1|\n|      01|     04|  Wireless Keyboard|       580|  1|\n|      02|     06|16 GB Flash Storage|       220|  1|\n|      01|     07|32 GB Flash Storage|       320|  2|\n|      04|     08|64 GB Flash Storage|       410|  2|\n+--------+-------+-------------------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# when having same name to multiple columns select will through ambiguity error as spark maps column names to internal ids while selecting columns\n",
    "# to resolve this there are two steps\n",
    "# 1. rename columns before joining\n",
    "# 2. remove anyone column after join\n",
    "\n",
    "product_renamed_df = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"inner\")\\\n",
    "    .drop(product_renamed_df.prod_id)\\\n",
    "    .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"qty\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7b1cc7-5515-4e77-a723-9a47ccdf6f03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----+-------------------+----------+-----------+\n|order_id|prod_id|unit_price| qty|          prod_name|list_price|reorder_qty|\n+--------+-------+----------+----+-------------------+----------+-----------+\n|    NULL|   NULL|      NULL|NULL|  Standard Keyboard|       360|         10|\n|      01|     02|       350|   1|      Optical Mouse|       350|         20|\n|      01|     04|       580|   1|  Wireless Keyboard|       580|         50|\n|      01|     07|       320|   2|32 GB Flash Storage|       320|         50|\n|      02|     03|       450|   1|     Wireless Mouse|       450|         50|\n|      02|     06|       220|   1|16 GB Flash Storage|       240|        100|\n|      03|     01|       195|   1|       Scroll Mouse|       250|         20|\n|      04|     08|       410|   2|64 GB Flash Storage|       430|         25|\n|      04|     09|       270|   3|               NULL|      NULL|       NULL|\n|      05|     02|       350|   1|      Optical Mouse|       350|         20|\n+--------+-------+----------+----+-------------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# OUTER JOIN\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"outer\")\\\n",
    "    .drop(product_renamed_df.prod_id)\\\n",
    "    .select(\"*\")\\\n",
    "    .sort(\"order_id\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5f17c4-14e3-41a0-93d2-9243f806d0b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+-------------------+----------+-----------+\n|order_id|prod_id|unit_price|qty|          prod_name|list_price|reorder_qty|\n+--------+-------+----------+---+-------------------+----------+-----------+\n|      01|     04|       580|  1|  Wireless Keyboard|       580|         50|\n|      01|     02|       350|  1|      Optical Mouse|       350|         20|\n|      01|     07|       320|  2|32 GB Flash Storage|       320|         50|\n|      02|     06|       220|  1|16 GB Flash Storage|       240|        100|\n|      02|     03|       450|  1|     Wireless Mouse|       450|         50|\n|      03|     01|       195|  1|       Scroll Mouse|       250|         20|\n|      04|     08|       410|  2|64 GB Flash Storage|       430|         25|\n|      04|     09|       270|  3|               NULL|      NULL|       NULL|\n|      05|     02|       350|  1|      Optical Mouse|       350|         20|\n+--------+-------+----------+---+-------------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# LEFT JOIN\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"left\")\\\n",
    "    .drop(product_renamed_df.prod_id)\\\n",
    "    .select(\"*\")\\\n",
    "    .sort(\"order_id\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae107332-699d-42ed-9a09-182cda71e43f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+-------------------+----------+-----------+\n|order_id|prod_id|unit_price|qty|          prod_name|list_price|reorder_qty|\n+--------+-------+----------+---+-------------------+----------+-----------+\n|      01|     07|       320|  2|32 GB Flash Storage|       320|         50|\n|      01|     02|       350|  1|      Optical Mouse|       350|         20|\n|      01|     04|       580|  1|  Wireless Keyboard|       580|         50|\n|      02|     06|       220|  1|16 GB Flash Storage|       240|        100|\n|      02|     03|       450|  1|     Wireless Mouse|       450|         50|\n|      03|     01|       195|  1|       Scroll Mouse|       250|         20|\n|      04|     09|       270|  3|                 09|       270|       NULL|\n|      04|     08|       410|  2|64 GB Flash Storage|       430|         25|\n|      05|     02|       350|  1|      Optical Mouse|       350|         20|\n+--------+-------+----------+---+-------------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# coleasce function\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "order_df.join(product_renamed_df, join_expr, \"left\")\\\n",
    "    .drop(product_renamed_df.prod_id)\\\n",
    "    .select(\"*\")\\\n",
    "    .withColumn(\"prod_name\", expr(\"coalesce(prod_name, prod_id)\"))\\\n",
    "    .withColumn(\"list_price\", expr(\"coalesce(list_price, unit_price)\"))\\\n",
    "    .sort(\"order_id\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0776c2-290b-491a-8021-e9f985557162",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revision\n",
    "\n",
    "\n",
    "# The list of joins provided by Spark SQL is:\n",
    "\n",
    "# Inner Join\n",
    "# Left / Left Outer Join\n",
    "# Right / Right Outer Join\n",
    "# Outer / Full Join\n",
    "# Cross Join\n",
    "# Left Anti Join\n",
    "# Left Semi Join\n",
    "# Self Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69cda460-b23f-4b2a-aec8-4eeb0581232f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Different Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716993ed-cdca-4157-8056-09525f00df25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+\n|order_id|prod_id|unit_price|qty|\n+--------+-------+----------+---+\n|      01|     02|       350|  1|\n|      01|     04|       580|  1|\n|      01|     07|       320|  2|\n|      02|     03|       450|  1|\n|      02|     06|       220|  1|\n|      03|     01|       195|  1|\n|      04|     09|       270|  3|\n|      04|     08|       410|  2|\n|      05|     02|       350|  1|\n+--------+-------+----------+---+\n\n+-------+-------------------+----------+---+\n|prod_id|          prod_name|list_price|qty|\n+-------+-------------------+----------+---+\n|     01|       Scroll Mouse|       250| 20|\n|     02|      Optical Mouse|       350| 20|\n|     03|     Wireless Mouse|       450| 50|\n|     04|  Wireless Keyboard|       580| 50|\n|     05|  Standard Keyboard|       360| 10|\n|     06|16 GB Flash Storage|       240|100|\n|     07|32 GB Flash Storage|       320| 50|\n|     08|64 GB Flash Storage|       430| 25|\n+-------+-------------------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "                .appName(\"Spark Joins\")\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "orders_list = [(\"01\", \"02\", 350, 1),\n",
    "                (\"01\", \"04\", 580, 1),\n",
    "                (\"01\", \"07\", 320, 2),\n",
    "                (\"02\", \"03\", 450, 1),\n",
    "                (\"02\", \"06\", 220, 1),\n",
    "                (\"03\", \"01\", 195, 1),\n",
    "                (\"04\", \"09\", 270, 3),\n",
    "                (\"04\", \"08\", 410, 2),\n",
    "                (\"05\", \"02\", 350, 1)]\n",
    "\n",
    "new_orders_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
    "new_df.show()\n",
    "\n",
    "product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
    "                (\"02\", \"Optical Mouse\", 350, 20), \n",
    "                (\"03\", \"Wireless Mouse\", 450, 50),\n",
    "                (\"04\", \"Wireless Keyboard\", 580, 50),\n",
    "                (\"05\", \"Standard Keyboard\", 360, 10),\n",
    "                (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
    "                (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
    "                (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
    "\n",
    "new_product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
    "new_product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0284b2c-9332-4dea-9680-b394dfd126f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+-------+-------------------+----------+---+\n|order_id|prod_id|unit_price|qty|prod_id|          prod_name|list_price|qty|\n+--------+-------+----------+---+-------+-------------------+----------+---+\n|      03|     01|       195|  1|     01|       Scroll Mouse|       250| 20|\n|      01|     02|       350|  1|     02|      Optical Mouse|       350| 20|\n|      05|     02|       350|  1|     02|      Optical Mouse|       350| 20|\n|      02|     03|       450|  1|     03|     Wireless Mouse|       450| 50|\n|      01|     04|       580|  1|     04|  Wireless Keyboard|       580| 50|\n|      02|     06|       220|  1|     06|16 GB Flash Storage|       240|100|\n|      01|     07|       320|  2|     07|32 GB Flash Storage|       320| 50|\n|      04|     08|       410|  2|     08|64 GB Flash Storage|       430| 25|\n+--------+-------+----------+---+-------+-------------------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Inner Join\n",
    "# Returns only the rows from both the dataframes that have matching values in both columns specified as the join keys.\n",
    "\n",
    "# Inner Join Example 1 -\n",
    "join_exp = new_orders_df.prod_id == new_product_df.prod_id\n",
    "\n",
    "new_orders_df.join(new_product_df, join_exp, \"inner\")\\\n",
    "                            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ea40c4-bbe4-4118-9af2-15e37dc54bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n+------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Inner Join Example 2 -\n",
    "\n",
    "df1 = spark.createDataFrame([(\"A\", 1), (\"B\", 2), (\"C\", 3)], [\"letter\", \"number\"])\n",
    "df2 = spark.createDataFrame([(\"A\", 4), (\"B\", 5), (\"D\", 6)], [\"letter\", \"value\"])\n",
    "\n",
    "join_exp_eg2 = df1['letter'] == df2['letter']\n",
    "\n",
    "df1.join(df2, join_exp_eg2, \"inner\")\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c11f1bf-6e58-45d7-8e76-10ffcc6c46e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n+------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Outer Join\n",
    "# Returns all the rows from the left dataframe and the matching rows from the right dataframe. If there are no matching values in the right dataframe, then it returns a null.\n",
    "\n",
    "df1.join(df2, df1['letter'] == df2['letter'], \"left\")\\\n",
    "                    .show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], \"leftouter\")\\\n",
    "                    .show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], \"left_outer\")\\\n",
    "                    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee72241-c729-4ccd-82ad-7b316f6f1671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Right Outer Join\n",
    "# Returns all the rows from the right dataframe and the matching rows from the left dataframe. If there are no matching values in the left dataframe, then it returns a null.\n",
    "\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'right').show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'rightouter').show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'right_outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2bf0ea-0381-44cf-9995-18d68bb74885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     B|     2|     B|    5|\n|     C|     3|  NULL| NULL|\n|  NULL|  NULL|     D|    6|\n+------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Outer / Full Join\n",
    "# Returns all the rows from both the dataframes, including the matching and non-matching rows. If there are no matching values, then the result will contain a NULL value in place of the missing data.\n",
    "\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'outer').show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'full').show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'fullouter').show()\n",
    "# OR\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'full_outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4a7c1f-fde9-4e81-bd70-4b955af596bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n|letter|number|letter|value|\n+------+------+------+-----+\n|     A|     1|     A|    4|\n|     A|     1|     B|    5|\n|     A|     1|     D|    6|\n|     B|     2|     A|    4|\n|     B|     2|     B|    5|\n|     B|     2|     D|    6|\n|     C|     3|     A|    4|\n|     C|     3|     B|    5|\n|     C|     3|     D|    6|\n+------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Cross Join\n",
    "# Returns all possible combinations of rows from both the dataframes. In other words, it takes every row from one dataframe and matches it with every row in the other dataframe. The result is a new dataframe with all possible combinations of the rows from the two input dataframes.\n",
    "\n",
    "df1.crossJoin(df2)\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46c5c0e-fe1e-491d-8553-ec33f1ebe0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|letter|number|\n+------+------+\n|     C|     3|\n+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Anti Join\n",
    "# A left anti join in Spark SQL is a type of left join operation that returns only the rows from the left dataframe that do not have matching values in the right dataframe. It is used to find the rows in one dataframe that do not have corresponding values in another dataframe.\n",
    "\n",
    "df1.join(df2, df1['letter'] == df2['letter'], 'left_anti')\\\n",
    "                        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9d2a93-9cc4-4755-baac-f4c146c8f1f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|letter|number|\n+------+------+\n|     A|     1|\n|     B|     2|\n+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Semi Join\n",
    "# A left semi join in Spark SQL is a type of join operation that returns only the columns from the left dataframe that have matching values in the right dataframe. It is used to find the values in one dataframe that have corresponding values in another dataframe.\n",
    "\n",
    "df1.join(df2, df1['letter'] == df2['letter'], \"leftsemi\")\\\n",
    "                            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2286a98-9ef3-4383-9156-b0b5ebb72834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n|letter|number|letter|number|\n+------+------+------+------+\n|     A|     1|     A|     1|\n|     B|     2|     B|     2|\n|     C|     3|     C|     3|\n+------+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Self Join\n",
    "# A self join in Spark SQL is a join operation in which a dataframe is joined with itself. It is used to compare the values within a single dataframe and return the rows that match specified criteria.\n",
    "\n",
    "df1.alias(\"temp1\").join(df1.alias(\"temp2\"), df1['letter'] == df1['letter'])\\\n",
    "                                        .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame Joins",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
